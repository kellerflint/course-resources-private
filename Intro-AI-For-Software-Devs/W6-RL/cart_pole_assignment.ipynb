{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - Reinforcement  Learning with CartPole\n",
    "\n",
    "In this notebook, we'll explore reinforcement learning (RL) using the CartPole environment from the Gymnasium library. RL is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties for its actions.\n",
    "\n",
    "The CartPole environment is a classic control problem where the agent (a cart) has to balance a pole upright by applying forces to the cart (left or right). The goal is to prevent the pole from falling and keep the cart within the track boundaries.\n",
    "\n",
    "First, let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell may take a few minutes to run depending on your system\n",
    "#!pip install gymnasium\n",
    "#!pip install stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the Gymnasium library for the CartPole environment and stable_baselines3 for the PPO RL algorithm.\n",
    "\n",
    "Gymnasium is a library that provides a collection of reinforcement learning environments that are easy for our agents to interact with. Stable_baselines3 is a library that contains implementations of state-of-the-art reinforcement learning algorithms and is a convenient tool for training and evaluating RL agents.\n",
    "\n",
    "Now, let's create the CartPole environment with a human-rendered mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a visual representation of the CartPole environment, allowing us to observe the agent's actions and the environment's state.\n",
    "\n",
    "Run the following code to observe the behavior of a random agent in the CartPole environment for 10 episodes. Observe how the cart and pole react to random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # Select a random action from the list of possible actions\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the cart and pole react to random actions. Notice that the pole falls quickly, and the cart moves randomly, unable to balance the pole.\n",
    "\n",
    "After seeing the random agent's behavior, we'll create a new environment without the human-rendered mode for faster training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training agents, let's explore the CartPole environment and understand its properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Environment Information:\")\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space represents the information the agent receives about the environment's state. The action space represents the actions the agent can take.\n",
    "\n",
    "This code resets the environment and prints the initial observation. The observation `obs` consists of the cart position, cart velocity, pole angle, and pole angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Observation:\n",
      "Cart Position 0.040557783\n",
      "Cart Velocity -0.033332333\n",
      "Pole Angle 0.030406915\n",
      "Pole Angular Velocity 0.012443069\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(\"\\nInitial Observation:\")\n",
    "print('Cart Position', obs[0][0])\n",
    "print('Cart Velocity', obs[0][1])\n",
    "print('Pole Angle', obs[0][2])\n",
    "print('Pole Angular Velocity', obs[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prints the available actions in the action space (0 and 1, representing left and right forces) and their data type (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "Action Data Type: int64\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\")\n",
    "print(\"Action Data Type:\", env.action_space.dtype)\n",
    "\n",
    "for action in range(env.action_space.n):\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a random agent for 10 episodes and record its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 17.0\n",
      "Episode: 2 Score: 38.0\n",
      "Episode: 3 Score: 14.0\n",
      "Episode: 4 Score: 13.0\n",
      "Episode: 5 Score: 14.0\n",
      "Episode: 6 Score: 12.0\n",
      "Episode: 7 Score: 40.0\n",
      "Episode: 8 Score: 18.0\n",
      "Episode: 9 Score: 10.0\n",
      "Episode: 10 Score: 21.0\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # Select a random action\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs a random agent for 10 episodes, selects random actions, and prints the score for each episode. The score represents the number of time steps that the pole remained balanced for.\n",
    "\n",
    "Now, we'll train a more intelligent agent using the Proximal Policy Optimization (PPO) RL algorithm. We need to start by creating a new environment and wrap it with DummyVecEnv to make it compatible with the stable_baselines3 library (don't worry about the details of this for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a PPO model with an MlpPolicy and specify the environment. MLP stands for multilayer perceptron. An MLP is a type of feedforward artificial neural network consisting of multiple layers of interconnected nodes (perceptrons).\n",
    "\n",
    "In the context of reinforcement learning, the MLP policy represents the agent's decision-making model. It takes the environment's state (observations) as input and outputs the probability distribution over actions. The MLP architecture allows the agent to learn complex patterns and relationships between the states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've established the environment and model, we can start training. This line trains the PPO model for 10,000 time steps, during which the agent learns to balance the pole by receiving rewards or penalties for its actions and ajusting the model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1011 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009234609 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.000863   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 57.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00860754 |\n",
      "|    clip_fraction        | 0.0535     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.666     |\n",
      "|    explained_variance   | 0.0858     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.8       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    value_loss           | 35.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 714         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016555637 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006187458 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a122406d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates the trained model's performance over 10 episodes and prints the evaluation results (mean reward, standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(\"Mean Reward:\", results[0])\n",
    "print(\"Standard Deviation\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** In the cell below, create a new PPO model and store it in the variable `model2`. Try out a different number of time steps during training and see how it impacts the mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model2 = \n",
    "\n",
    "# train the model\n",
    "\n",
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the trained agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "episodes = 3\n",
    "max_duration = 200\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    i = 0\n",
    "    while not done and i < max_duration:\n",
    "        i = i + 1\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the trained agent for 3 episodes then prints the score for each episode. We also limit each episode to a max duration of 200 time steps. You should observe that the trained agent can balance the pole for longer periods compared to the random agent.\n",
    "\n",
    "**Your Turn**: Run the visualization loop using your `model2` agent to predict the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "episodes = 3\n",
    "max_duration = 200\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    i = 0\n",
    "    while not done and i < max_duration:\n",
    "        i = i + 1\n",
    "        env.render()\n",
    "        action, _ = # predict the next action using your model\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
