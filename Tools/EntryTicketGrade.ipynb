{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- You're going to need to have the questions labeled in the question text (like written response or something vs multiple choice, maybe with ** to denote it).\n",
    "- I think the other thing you should do is in the question, clearly state the rubric / requirements right. Maybe just in small text at the bottom or something so the AI can go off of the actual criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_KEY = os.getenv(\"GROQ_KEY\")\n",
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_KEY,\n",
    ")\n",
    "\n",
    "def call_groq(query):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You will be given a question and a set of responses to that question. Go through each response and evaluate each for correctness. Decide if they are \\n3/3: Largely Correct \\n2/3: Moderate mistakes / misunderstandings \\n1/3: Mostly incorrect \\n0/3: Not attempted or incoherent. \\nProvide your response as a json object with the response number. For example, [{'id': 1, 'response': '<full text of their response>', 'eval': 'Your eval / feedback here...', 'score': 3}, {'id': 2, 'response': '<full text of their response>', 'eval': 'Your eval / feedback here...', 'score': 1}]\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% training / 10% testing: In this option, this model will likely have a more general familiarity with the training dataset and when encountering new and unseen datasets, while the other option will have a lack of accuracy and will be familiar with unseen datasets. In conclusion, while they have their advantages and disadvantages, a banlance between testing and training will give out a better model.\n",
      "Smaller training set would result on the model to learn only a small portion of the data, thus when testing on unseen data it would have a higher MSE value. A larger training set would reduce the likeliness of that happening but with a smaller testing set it wouldn't  be able to test on a larger range of data\n",
      "Using a larger training dataset (90 / 10) can be beneficial because the model can have a better idea of the trends of the dataset. It can perform \"well\" with interpreting the data, thus creating better generalizations and trends. The disadvantages to this is that it may overfit the data. If it's been trained on a lot of data, then it may easily identify trends. This can be an issue if the model has not seen data it has recognized. \n",
      "\n",
      "With using a smaller training set dataset (10 / 90), the disadvantage to this is that it may not capture all data points and possibilities, which makes it difficult to identify trends and create generalizations. \n",
      "Overfitting could be a disadvantage, the model could be learning too well since it would have access to 90% of the existing data. But an advantage could be less margin of error. The model does not have a lot of room for inferring since It would be fairly accurate having seen most of the data with only 10% of it being unseen.\n",
      "When training with a larger proportion of the data, one advantage might be how robustly the model has been trained. If less of the data is reserved for training, then there might not be enough data to test the model. Patterns can form when training models, bagging and bootstrapping can help ensure accuracy between models. Less data for training can lead to underfit models.\n",
      "The advantages of using a larger training set would be that the model would have more data to train on and thereby make accurate predictions. However, since you have a much smaller testing set as a result, there may not be enough data to accurately test the model which may have learned biases/overfitted. If you have a smaller training set and large testing, then you probably just wouldn't have enough data to accurately train the model to start even though you would have a plethora of training data to test it whatever predictions it could make.\n",
      "Some advantages of a having a higher split for training data is just that, the model is able to train on more data, and typically the more data you provide a model the better it can pick out patterns. A drawback to this is the chance of overfitting on this training data, which would result in poor results when evaluating the model on the test results. On the flip side, if we provided the model with more testing data than training, the model may struggle to perform because it just hasn't seen enough data. Other factors that play into this is how much data you have to start out with, and the goal of the model.\n",
      "The proportion chosen may impact the model's accuracy when trained and will likely impact the ability to generalize using new test cases. The typical 80/20 split provides a good balance between teaching the model using enough data while preserving a significant number of records to test with. The higher the percentage of the data used for testing, the more value the testing can have because it represents a more extensive test case. Conversely, reducing the training data set may result in a less thoroughly trained model, meaning our error rate will be higher when using the model to generate predictions.\n",
      "Overall, the split should be enough to allow the model to generalize well while preserving a large enough test set to generate a representative test that provides enough data. Too large a training set will likely result in an overfit, and too small a training set will likely result in an underfit.\n",
      "The benefits of using a large training dataset allow the model to have access to a large amount of data to learn and pull from. However, this can lead to it not being able to do specific tasks better when testing. On the other side, if you don't give the model enough training data and mostly give it test material, it will only do well at very specific tests without much outside data to pull from.\n",
      "The advantages of using a larger data set is better training, better stability, and reduces overfitting. With a larger set it allows the model to recognize more patterns which in turn increases performance. It allows the data to have more examples at it's disposal and a larger data set will allow it to be more consistent with it's logic. The disadvantages of using a larger training set is less testing since only 10% is used for testing, and the ability to generalize. Since there is less data for testing, there is less chances to evaluate the models actual performance which could make the model less reliable and not good with specific scenarios.\n",
      "Less data to train with may result in not getting accurate results. \n",
      "\n",
      "Too little testing data may result in insufficient testing analysis\n",
      "\n",
      " \n",
      "\n",
      "More training data improves the likelihood of having a well trained model, but if we don't have enough data to test we may not be able to effectively assess the accuracy of the model. \n",
      "\n",
      "Less data for training can result in an insufficiently trained model, but allows us a high degree of effectiveness in model evaluation.\n",
      "The advantages to using a larger training data set is that it gives your model more training data with which to develop a model and could improve it's accuracy. This could be important if you don't have large amounts of data to work with to begin with, or if your data has great diversity that it is important to capture trends in.   This means however you have smaller amounts of test data with which to analyze the performance of your model.   If you have a smaller training set relative to testing data then if your data is relatively homogenous and your training set is still large enough to generate a good model after the split you have much more flexibility and opportunity to do testing with a variety of testing data to properly validate your model. You can be much more confident about the performance of your model in this scenario. \n",
      "When using a large training set (90% training and 10% testing), the data will likely be overfitted towards the training data. The small nuisances and patterns will be adapted by the model which makes it not as good when analyzing a more generalized dataset. So in this case, when using the model on the 10% test data , it will show a big inaccuracies due to model overfitting the training data.\n",
      "\n",
      "And vice versa for the opposite (10% training and 90% test). There isn't enough data here to train the model to detect the more subtle patterns in the data so, when presented with a large amount of test data, the model will underfit. \n",
      "Smaller training sets may provide an early advantage in detecting overfitting within a test. Although this is a good benefit, the smaller training set could affect the model's predictive accuracy. On the other hand, a larger training set would reduce the risk of underfitting. However, the smaller testing set means less data and may not represent that data as a whole very well.\n",
      "When training a dataset, you want to keep your model from overfitting to the training data. This means having a variety in the data and not having too small of a data set. But it's also best to make sure you're not training on too much information. You also need to keep some data for testing that the model hasn't seen before. It is better to split the data 50/50 to keep the training and testing as similar as possible to determine if the model is overfitting or underfitting the data.\n",
      "With every data point you add, assuming there is a true correlation, you get closer to an accurate regression model.\n",
      "\n",
      "Using a large amount of data from the same set and testing it against a small portion of the remaining set can help gauge your models accuracy and precision.\n",
      "\n",
      "This is a weird analogy, but consider it like a rifle. The longer the barrel, the more accurate the shot. In this analogy, the training set is the length of the barrel, and the testing set is whatever the target is.\n",
      "The advantages of doing 90% training / 10% testing is the model will have a lot of data to train on so it will hopefully be more accurate the disadvantage to this is it will not have a lot of data to test on so the results might be affected significantly by a small amount of the data. For 10%/ training 90% testing the advantages are that you will be able to test your data and get a good overall result of the entire dataset, for the testing though its such small amount of data to train on the model will only be able to learn from a small amount of data skewing the accuracy. \n",
      "A larger training set (such as 90% training/ 10% testing) can help fit the model more effectively to the data, but the issue is that the model can be too well-fitted, leading to overfitting where the model struggles to perform well outside of its training data; the flipside of this is underfitting, where the model doesn't fit well to the data it's trained on due to not being provided with enough data, leading to chaotic predictions that may be completely off-base from what should be expected given the data. At the same time, having a lower ratio of train to test splitting could help the data be less prone to overfitting; it's a tough balance to strike, where one needs to train the model with the right amount of data without \"putting it in a box\" where it can only ever answer based on that data and not have room to actually learn and grow.\n",
      "\n",
      "Hopefully that makes sense.\n",
      "With a larger training set, the model is less likely to have overfitting due to having more data to pull from. However, it'll also produce less accurate test results due to having less data to test with. With a smaller training set, the model may be good at predicting outcomes for only that specific dataset, however it'll have more accurate test results. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kflin\\AppData\\Local\\Temp\\ipykernel_2392\\3528460215.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(row[13])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Submissions/M5Entry.csv')\n",
    "df.head()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>177945147: **Written Response** When splitting a dataset into training and testing subsets, you have the option to choose different proportions of the data for training vs testing. Discuss the advantages and disadvantages of using a larger training set (such as 90% training / 10% testing) compared to a smaller training set (like 10% training / 90% testing).</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90% training / 10% testing: In this option, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smaller training set would result on the model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Using a larger training dataset (90 / 10) can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overfitting could be a disadvantage, the model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When training with a larger proportion of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  177945147: **Written Response** When splitting a dataset into training and testing subsets, you have the option to choose different proportions of the data for training vs testing. Discuss the advantages and disadvantages of using a larger training set (such as 90% training / 10% testing) compared to a smaller training set (like 10% training / 90% testing).\n",
       "0  90% training / 10% testing: In this option, th...                                                                                                                                                                                                                                                                                                                     \n",
       "1  Smaller training set would result on the model...                                                                                                                                                                                                                                                                                                                     \n",
       "2  Using a larger training dataset (90 / 10) can ...                                                                                                                                                                                                                                                                                                                     \n",
       "3  Overfitting could be a disadvantage, the model...                                                                                                                                                                                                                                                                                                                     \n",
       "4  When training with a larger proportion of the ...                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find index of column with 'Written Response'\n",
    "written_index = [i for i in range(len(df.columns)) if '**Written Response**' in df.columns[i]]\n",
    "print(written_index)\n",
    "\n",
    "# Keep only the columns with 'Written Response'\n",
    "df = df.iloc[:, written_index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the evaluations of each response:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"response\": \"90% training / 10% testing: In this option, this model will likely have a more general familiarity with the training dataset and when encountering new and unseen datasets, while the other option will have a lack of accuracy and will be familiar with unseen datasets. In conclusion, while they have their advantages and disadvantages, a banlance between testing and training will give out a better model.\",\n",
      "    \"eval\": \"The response touches on the idea of a balance between training and testing, but it's not clear what the advantages and disadvantages of each option are. The response also seems to contradict itself, stating that the 90% training option will have a 'more general familiarity' with the training dataset, but then saying that the 10% training option will be 'familiar with unseen datasets'.\",\n",
      "    \"score\": 1\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"response\": \"Smaller training set would result on the model to learn only a small portion of the data, thus when testing on unseen data it would have a higher MSE value. A larger training set would reduce the likeliness of that happening but with a smaller testing set it wouldn’t be able to test on a larger range of data\",\n",
      "    \"eval\": \"The response correctly identifies the potential issue with a smaller training set (higher MSE value) and the potential benefit of a larger training set (reduced MSE value). However, it doesn't fully explore the trade-offs between the two options.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"response\": \"Using a larger training dataset (90 / 10) can be beneficial because the model can have a better idea of the trends of the dataset. It can perform 'well' with interpreting the data, thus creating better generalizations and trends. The disadvantages to this is that it may overfit the data. If it's been trained on a lot of data, then it may easily identify trends. This can be an issue if the model has not seen data it has recognized. With using a smaller training set dataset (10 / 90), the disadvantage to this is that it may not capture all data points and possibilities, which makes it difficult to identify trends and create generalizations.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (better generalizations and trends) and the potential drawbacks (overfitting). It also correctly identifies the potential issue with a smaller training set (difficulty in identifying trends and creating generalizations).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"response\": \"Overfitting could be a disadvantage, the model could be learning too well since it would have access to 90% of the existing data. But an advantage could be less margin of error. The model does not have a lot of room for inferring since It would be fairly accurate having seen most of the data with only 10% of it being unseen.\",\n",
      "    \"eval\": \"The response correctly identifies the potential issue with a larger training set (overfitting) and the potential benefit (less margin of error). However, it doesn't fully explore the trade-offs between the two options.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 5,\n",
      "    \"response\": \"When training with a larger proportion of the data, one advantage might be how robustly the model has been trained. If less of the data is reserved for training, then there might not be enough data to test the model. Patterns can form when training models, bagging and bootstrapping can help ensure accuracy between models. Less data for training can lead to underfit models.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefit of a larger training set (robust training) and the potential issue with a smaller training set (underfitting). However, it doesn't fully explore the trade-offs between the two options.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 6,\n",
      "    \"response\": \"The advantages of using a larger training set would be that the model would have more data to train on and thereby make accurate predictions. However, since you have a much smaller testing set as a result, there may not be enough data to accurately test the model which may have learned biases/overfitted. If you have a smaller training set and large testing, then you probably just wouldn't have enough data to accurately train the model to start even though you would have a plethora of training data to test it whatever predictions it could make.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (accurate predictions) and the potential drawbacks (overfitting and limited testing data). It also correctly identifies the potential issue with a smaller training set (difficulty in training the model).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 7,\n",
      "    \"response\": \"Some advantages of a having a higher split for training data is just that, the model is able to train on more data, and typically the more data you provide a model the better it can pick out patterns. A drawback to this is the chance of overfitting on this training data, which would result in poor results when evaluating the model on the test results. On the flip side, if we provided the model with more testing data than training, the model may struggle to perform because it just hasn't seen enough data. Other factors that play into this is how much data you have to start out with, and the goal of the model.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (better pattern recognition) and the potential drawbacks (overfitting). It also correctly identifies the potential issue with a smaller training set (difficulty in training the model).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 8,\n",
      "    \"response\": \"The proportion chosen may impact the model's accuracy when trained and will likely impact the ability to generalize using new test cases. The typical 80/20 split provides a good balance between teaching the model using enough data while preserving a significant number of records to test with. The higher the percentage of the data used for testing, the more value the testing can have because it represents a more extensive test case. Conversely, reducing the training data set may result in a less thoroughly trained model, meaning our error rate will be higher when using the model to generate predictions. Overall, the split should be enough to allow the model to generalize well while preserving a large enough test set to generate a representative test that provides enough data. Too large a training set will likely result in an overfit, and too small a training set will likely result in an underfit.\",\n",
      "    \"eval\": \"The response correctly identifies the potential impact of the proportion of training and testing data on the model's accuracy and ability to generalize. It also correctly identifies the potential benefits and drawbacks of different splits (e.g. 80/20, too large a training set, too small a training set).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 9,\n",
      "    \"response\": \"The benefits of using a large training dataset allow the model to have access to a large amount of data to learn and pull from. However, this can lead to it not being able to do specific tasks better when testing. On the other side, if you don't give the model enough training data and mostly give it test material, it will only do well at very specific tests without much outside data to pull from.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (access to more data) and the potential drawbacks (difficulty in performing specific tasks). However, it doesn't fully explore the trade-offs between the two options.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 10,\n",
      "    \"response\": \"The advantages of using a larger data set is better training, better stability, and reduces overfitting. With a larger set it allows the model to recognize more patterns which in turn increases performance. It allows the data to have more examples at it's disposal and a larger data set will allow it to be more consistent with it's logic. The disadvantages of using a larger training set is less testing since only 10% is used for testing, and the ability to generalize. Since there is less data for testing, there is less chances to evaluate the models actual performance which could make the model less reliable and not good with specific scenarios.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (better training, stability, and pattern recognition) and the potential drawbacks (less testing data and difficulty in generalizing). However, it incorrectly states that a larger training set reduces overfitting.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 11,\n",
      "    \"response\": \"Less data to train with may result in not getting accurate results. Too little testing data may result in insufficient testing analysis More training data improves the likelihood of having a well trained model, but if we don't have enough data to test we may not be able to effectively assess the accuracy of the model. Less data for training can result in an insufficiently trained model, but allows us a high degree of effectiveness in model evaluation.\",\n",
      "    \"eval\": \"The response correctly identifies the potential issues with a smaller training set (inaccurate results) and a smaller testing set (insufficient testing analysis). It also correctly identifies the potential benefits of a larger training set (well-trained model) and a larger testing set (effective model evaluation).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 12,\n",
      "    \"response\": \"The advantages to using a larger training data set is that it gives your model more training data with which to develop a model and could improve it's accuracy. This could be important if you don't have large amounts of data to work with to begin with, or if your data has great diversity that it is important to capture trends in. This means however you have smaller amounts of test data with which to analyze the performance of your model. If you have a smaller training set relative to testing data then if your data is relatively homogenous and your training set is still large enough to generate a good model after the split you have much more flexibility and opportunity to do testing with a variety of testing data to properly validate your model. You can be much more confident about the performance of your model in this scenario.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (improved accuracy) and the potential drawbacks (less testing data). It also correctly identifies the potential benefits of a smaller training set (more flexibility in testing) and the importance of considering the diversity of the data.\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 13,\n",
      "    \"response\": \"When using a large training set (90% training and 10% testing), the data will likely be overfitted towards the training data. The small nuisances and patterns will be adapted by the model which makes it not as good when analyzing a more generalized dataset. So in this case, when using the model on the 10% test data , it will show a big inaccuracies due to model overfitting the training data. And vice versa for the opposite (10% training and 90% test). There isn't enough data here to train the model to detect the more subtle patterns in the data so, when presented with a large amount of test data, the model will underfit.\",\n",
      "    \"eval\": \"The response correctly identifies the potential issue with a larger training set (overfitting) and the potential issue with a smaller training set (underfitting).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 14,\n",
      "    \"response\": \"Smaller training sets may provide an early advantage in detecting overfitting within a test. Although this is a good benefit, the smaller training set could affect the model's predictive accuracy. On the other hand, a larger training set would reduce the risk of underfitting. However, the smaller testing set means less data and may not represent that data as a whole very well.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a smaller training set (early detection of overfitting) and the potential drawbacks (reduced predictive accuracy). It also correctly identifies the potential benefits of a larger training set (reduced risk of underfitting) and the potential drawbacks (less representative testing data).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 15,\n",
      "    \"response\": \"When training a dataset, you want to keep your model from overfitting to the training data. This means having a variety in the data and not having too small of a data set. But it's also best to make sure you're not training on too much information. You also need to keep some data for testing that the model hasn't seen before. It is better to split the data 50/50 to keep the training and testing as similar as possible to determine if the model is overfitting or underfitting the data.\",\n",
      "    \"eval\": \"The response correctly identifies the importance of avoiding overfitting and underfitting, and the need to balance the training and testing data. However, it doesn't fully explore the trade-offs between different splits (e.g. 90/10, 50/50).\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 16,\n",
      "    \"response\": \"With every data point you add, assuming there is a true correlation, you get closer to an accurate regression model. Using a large amount of data from the same set and testing it against a small portion of the remaining set can help gauge your models accuracy and precision. This is a weird analogy, but consider it like a rifle. The longer the barrel, the more accurate the shot. In this analogy, the training set is the length of the barrel, and the testing set is whatever the target is.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefit of a larger training set (improved accuracy) and the importance of testing the model. However, it doesn't fully explore the trade-offs between different splits (e.g. 90/10, 50/50).\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": 17,\n",
      "    \"response\": \"The advantages of doing 90% training / 10% testing is the model will have a lot of data to train on so it will hopefully be more accurate the disadvantage to this is it will not have a lot of data to test on so the results might be affected significantly by a small amount of the data. For 10%/ training 90% testing the advantages are that you will be able to test your data and get a good overall result of the entire dataset, for the testing though its such small amount of data to train on the model will only be able to learn from a small amount of data skewing the accuracy.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (improved accuracy) and the potential drawbacks (less testing data). It also correctly identifies the potential benefits of a larger testing set (good overall result) and the potential drawbacks (small amount of training data).\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 18,\n",
      "    \"response\": \"A larger training set (such as 90% training/ 10% testing) can help fit the model more effectively to the data, but the issue is that the model can be too well-fitted, leading to overfitting where the model struggles to perform well outside of its training data; the flipside of this is underfitting, where the model doesn't fit well to the data it's trained on due to not being provided with enough data, leading to chaotic predictions that may be completely off-base from what should be expected given the data. At the same time, having a lower ratio of train to test splitting could help the data be less prone to overfitting; it's a tough balance to strike, where one needs to train the model with the right amount of data without 'putting it in a box' where it can only ever answer based on that data and not have room to actually learn and grow.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (better fit to the data) and the potential drawbacks (overfitting). It also correctly identifies the potential issue with a smaller training set (underfitting) and the importance of striking a balance between training and testing data.\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  {\n",
      "    \"id\": 19,\n",
      "    \"response\": \"With a larger training set, the model is less likely to have overfitting due to having more data to pull from. However, it'll also produce less accurate test results due to having less data to test with. With a smaller training set, the model may be good at predicting outcomes for only that specific dataset, however it'll have more accurate test results.\",\n",
      "    \"eval\": \"The response correctly identifies the potential benefits of a larger training set (reduced overfitting) and the potential drawbacks (less accurate test results). However, it incorrectly states that a smaller training set will produce more accurate test results.\",\n",
      "    \"score\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# iterate through and call a funcion each row call a function called 'score' that takes the question and written response as input and returns a score\n",
    "def score_question(question, responses):\n",
    "    # Call the GROQ API\n",
    "    query = f\"Question: '{question}':\\n\\nResponses:\"\n",
    "    for i, response in enumerate(responses):\n",
    "        query += f\"\\n\\n{i+1}:\\n{response}\"\n",
    "    #print(query)\n",
    "    response = call_groq(query)\n",
    "    print(response)\n",
    "\n",
    "# from written_indexes. There are multiple. get each one for each question and written response\n",
    "for i in range(len(written_index)):\n",
    "    question = df.columns[i]\n",
    "    responses = df.iloc[:, i].tolist()\n",
    "    score_question(question, responses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
